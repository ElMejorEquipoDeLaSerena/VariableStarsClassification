{
 "metadata": {
  "name": "",
  "signature": "sha256:503b4985826ac17dbd4b60697bf53ee32d4d7158b52eba82ae65576cf75989b3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os.path\n",
      "import sys\n",
      "import urllib\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import pylab as pl\n",
      "\n",
      "from sklearn import svm, cross_validation, metrics\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "from astroML.time_series import lomb_scargle\n",
      "\n",
      "# Set some default font properties\n",
      "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
      "matplotlib.rcParams['font.size'] = 18"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Load Data\n",
      "\n",
      "Our catalog data comes mostly from the Catalina Sky Survey (http://catalinadata.org) but also includes additional data from other catalogs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load our matched data from the catalog\n",
      "cat_data = 'catalog_data_final/matched_data.csv'\n",
      "catalog = np.genfromtxt(cat_data, dtype=None, names=True, delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Columns in our catalog:\"\n",
      "for i, name in enumerate(catalog.dtype.names):\n",
      "    print \"{:2d} {}\".format(i, name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Columns in our catalog:\n",
        " 0 InputID\n",
        " 1 ID\n",
        " 2 Mag\n",
        " 3 Magerr\n",
        " 4 RA\n",
        " 5 Decl\n",
        " 6 MJD\n",
        " 7 Blend\n",
        " 8 Catalina_Surveys_ID\n",
        " 9 Numerical_ID\n",
        "10 RA_J2000\n",
        "11 Dec\n",
        "12 V_mag\n",
        "13 Period_days\n",
        "14 Amplitude\n",
        "15 Number_Obs\n",
        "16 Var_Type\n",
        "17 epoch_folding\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Setup our Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_irq(rows):\n",
      "    mags = rows['Mag']\n",
      "    return np.percentile(mags, 75) - np.percentile(mags, 25)\n",
      "\n",
      "def get_skew(rows):\n",
      "    skew = stats.skew(rows['Mag'])    \n",
      "    return skew\n",
      "\n",
      "def get_median(rows):\n",
      "    return np.median(rows['Mag'])\n",
      "\n",
      "def build_features(save_file='features.txt', limit=False):\n",
      "    \"\"\"\n",
      "    Builds our feature array. Loops over entire loaded catalog\n",
      "    and creates one row per object. Current features include:\n",
      "    \n",
      "        * Period (from catalog)\n",
      "        * Magnitude\n",
      "        * Median\n",
      "        * Amplitude\n",
      "        * Skew\n",
      "        * IQR\n",
      "        \n",
      "    If you want a limited set (for faster building), pass\n",
      "    limit=True\n",
      "    \"\"\"\n",
      "\n",
      "    obj_id_array = np.unique(catalog['ID'])\n",
      "\n",
      "    features = np.zeros( (len(obj_id_array), 10) )\n",
      "\n",
      "    # For each object\n",
      "    for i in range(len(obj_id_array)):\n",
      "        obj_id = obj_id_array[i]\n",
      "    \n",
      "        rows = catalog[(catalog['ID'] == obj_id)]\n",
      "        \n",
      "        if limit & i > 500:\n",
      "            break\n",
      "        \n",
      "        features[i,0] = obj_id # Object ID\n",
      "        features[i,1] = rows['Var_Type'][0] # Class\n",
      "        features[i,2] = rows['RA'][0] # RA_J2000\n",
      "        features[i,3] = rows['Decl'][0] # Dec\n",
      "        features[i,4] = rows['Period_days'][0]  # Period\n",
      "        features[i,5] = rows['Mag'][0]  # Mag\n",
      "        features[i,6] = get_median(rows)  # Median\n",
      "        features[i,7] = rows['Amplitude'][0]  # Amplitude\n",
      "        features[i,8] = get_skew(rows)  # Skew\n",
      "        features[i,9] = get_irq(rows)  # IQR\n",
      "        \n",
      "        # Save the files\n",
      "        np.savetxt('features.txt', features, delimiter=',', \n",
      "           header=\"Object ID, Class, RAJ2000, Dec, Period, Mag, Amplitude, Median, Skew, IQR\")\n",
      "        \n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get our features. Either build them from scratch or load from file\n",
      "# If we build them from scratch, we also pass in the file name to save.\n",
      "\n",
      "features = None\n",
      "\n",
      "features_file = 'features.txt'\n",
      "if os.path.isfile(features_file):\n",
      "    features = np.loadtxt(features_file, dtype=None, delimiter=\",\")\n",
      "else:\n",
      "    features = build_features(save_file=features_file)\n",
      "    \n",
      "# Display a row the features\n",
      "features[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([  1.00100400e+12,   3.00000000e+00,   1.07909400e+01,\n",
        "        -2.64900000e+00,   2.18273000e-01,   1.73200000e+01,\n",
        "         1.73000000e+01,   6.00000000e-01,   1.24340797e+00,\n",
        "         2.20000000e-01])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Separate Training and Test Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Start to build the table we will use\n",
      "\n",
      "# For Training data, we want to get a certain amount of our\n",
      "# table, we slice off a percentage\n",
      "row_start = 0\n",
      "row_end = len(features) - int(.25 * len(features))\n",
      "\n",
      "# Our features table also contains meta information, so we slice\n",
      "feature_column_start = 4\n",
      "\n",
      "# Our collections of data to train and test\n",
      "labeled_svm = features[row_start:row_end,feature_column_start:]\n",
      "unlabeled_svm = features[row_end:,feature_column_start:]\n",
      "\n",
      "# The correct classes corresponding to the training and test\n",
      "classes = features[row_start:row_end,1]\n",
      "unlabeled_classes = features[row_end:,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Display the number of each class that we have in the training and test data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "types = { 1: 'EW', 2: 'EA', 3: 'beta Lyrae'}\n",
      "\n",
      "# How many of each Variable Type do we have\n",
      "print \"Type\\t\\t\\tType\\t\\tNumber\".format()\n",
      "print \"{}\".format('-'*45)\n",
      "for x in range(1,4):\n",
      "    print \"Labeled Class size:\\t {:14s} {}\".format(types[x],len(classes[classes == x]))\n",
      "    print \"Unlabelel Class size:\\t {:14s} {}\\n\".format(types[x],len(unlabeled_classes[unlabeled_classes == x]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Type\t\t\tType\t\tNumber\n",
        "---------------------------------------------\n",
        "Labeled Class size:\t EW             2888\n",
        "Unlabelel Class size:\t EW             1036\n",
        "\n",
        "Labeled Class size:\t EA             450\n",
        "Unlabelel Class size:\t EA             85\n",
        "\n",
        "Labeled Class size:\t beta Lyrae     37\n",
        "Unlabelel Class size:\t beta Lyrae     4\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Figure out what training size is optimal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we use a set Test size but are trying to determine the optimal size for our training set. We loop over a number of sizes and determine the accuracy from each."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we do an initial split on the data and then try to predict with our classifier. Because of the simple split done with `train_test_split` we do not expect to achieve a high accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create our (linear) classifier\n",
      "clf = svm.LinearSVC()\n",
      "clf.fit(labeled_svm, classes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split (labeled_svm, classes, test_size=1./3.)\n",
      "\n",
      "print(\"training set = {} {}\".format(X_train.shape, y_train.shape))\n",
      "print(\"test size = {} {}\".format(X_test.shape, y_test.shape))\n",
      "\n",
      "clf.fit(X_train, y_train)\n",
      "pred_class = clf.predict(X_test)\n",
      "N_match = (pred_class == y_test).sum()\n",
      "print(\"N_match = {}\".format(N_match))\n",
      "acc = 1. * N_match / len(pred_class)\n",
      "print(\"Accuracy = {}\".format(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training set = (2250, 6) (2250,)\n",
        "test size = (1125, 6) (1125,)\n",
        "N_match = 1028"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy = 0.913777777778\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we use a slightly more intelligent split of the labeled data (with `StratifiedShuffleSplit`) to attempt to achieve a higher accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ss = cross_validation.StratifiedShuffleSplit(classes, 5, test_size = 1./3.)\n",
      "scores = cross_validation.cross_val_score(clf, labeled_svm, classes, cv=ss)\n",
      "print \"Accuracy = {} +- {}\".format(scores.mean(),scores.std())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy = 0.915377777778 +- 0.00329728657618\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### See if there is a better training size that we can use"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N_test = 250\n",
      "\n",
      "step = 100\n",
      "Ns = np.arange(500, len(labeled_svm)-(2* step), step)\n",
      "\n",
      "#print \"Attempted N sizes = {}\".format(Ns)\n",
      "scores = np.zeros(len(Ns))\n",
      "stds = np.zeros(len(Ns))\n",
      "for i in range(len(Ns)):\n",
      "    N = Ns[i]    \n",
      "    ss = cross_validation.StratifiedShuffleSplit(classes, 5, test_size = N_test, train_size = N)\n",
      "    scores_i = cross_validation.cross_val_score(clf, labeled_svm, classes, cv=ss)\n",
      "    scores[i] = scores_i.mean()\n",
      "    stds[i] = scores_i.std()\n",
      "\n",
      "    \n",
      "# Get our optimal n\n",
      "optimal_n = Ns[np.argmax(scores)]\n",
      "    \n",
      "# Plot our results, including optimal n\n",
      "pl.clf()\n",
      "fig = pl.figure()\n",
      "ax = fig.add_subplot(1,1,1)\n",
      "ax.errorbar (Ns, scores, yerr = stds)\n",
      "ax.set_xlabel(\"N\")\n",
      "ax.set_ylabel(\"Accuracy\")\n",
      "pl.title(\"Optimal: $n={}$\".format(optimal_n))    \n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-1b3c2d3b43ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mscores_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_svm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/wtgee/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, score_func, pre_dispatch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         delayed(_cross_val_score)(clone(estimator), X, y, scorer, train, test,\n\u001b[0;32m   1151\u001b[0m                                   verbose, fit_params)\n\u001b[1;32m-> 1152\u001b[1;33m         for train, test in cv)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/wtgee/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/wtgee/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/wtgee/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/wtgee/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_cross_val_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, fit_params)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m     \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscorer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/wtgee/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    690\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                                               rnd.randint(np.iinfo('i').max))\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Figure out the optimal C\n",
      "\n",
      "As an exercise, we compute the optimal $C$ scaling parameter for the equation below. We are still using the linear SVC so this is mostly an example of how one would compute this parameter. A graph of the possible $C$ values are displayed as well as the best estimator. \n",
      "\n",
      "$\\min_{\\bf{w}, \\xi, b}\\left\\{\\frac{1}{2}||\\bf{w}||^2 + C\\sum_{i=1}^n\\xi_i\\right\\}$\n",
      "subject to $y_i(\\bf{w\\cdot x}_i-b)\\geq 1-\\xi_i$, $\\xi_i\\geq 0$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use our optimal_n from above as our training size\n",
      "N_train = optimal_n\n",
      "\n",
      "C_range = 10. ** np.arange(-5, 5)\n",
      "param_grid = dict(C=C_range)\n",
      "ss = cross_validation.StratifiedShuffleSplit(classes, 5, test_size = N_test, train_size = N_train)\n",
      "grid = GridSearchCV(svm.LinearSVC(), param_grid=param_grid, cv=ss)\n",
      "grid.fit (labeled_svm, classes)\n",
      "\n",
      "# plot the scores of the grid grid_scores_ contains parameter settings and scores\n",
      "# grid_scores_ contains parameter settings and scores                                                 \n",
      "score_dict = grid.grid_scores_\n",
      "# We extract just the scores\n",
      "scores = [x[1] for x in score_dict]\n",
      "\n",
      "pl.clf()\n",
      "fig = pl.figure()\n",
      "ax = fig.add_subplot(1,1,1)\n",
      "ax.plot (C_range, scores)\n",
      "ax.set_xscale(\"log\")\n",
      "ax.set_xlabel(\"$C$\")\n",
      "ax.set_ylabel(\"Accuracy\")\n",
      "ax.set_title(\"Optimal: $C={}$\".format(grid.best_estimator_.C))\n",
      "pl.show ()\n",
      "\n",
      "print \"The best classifier is: {}\".format(grid.best_estimator_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Support Vector Classification (SVC)\n",
      "\n",
      "We use the sklean.svm.svc module with a few different kernels to attempt some fits.\n",
      "\n",
      "The equation governing this classification is:\n",
      "\n",
      "$K(\\bf{x}, \\bf{x}') = e^{-\\gamma||\\bf{x}-\\bf{x}'||^2}$\n",
      "   \n",
      "Threfore we need to obatin both $C$ and $\\gamma$, so we test a range of values and create a heat-map to show optimal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_optimal_svc(param_grid=list()):\n",
      "    \n",
      "    # Do the grid search\n",
      "    grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=ss)\n",
      "    grid.fit (labeled_svm, classes)\n",
      "    score_dict = grid.grid_scores_\n",
      "\n",
      "    # We extract the scores\n",
      "    scores = [x[1] for x in score_dict]\n",
      "    scores = np.array(scores).reshape(len(C_range), len(gamma_range))\n",
      "\n",
      "    # Make a nice figure\n",
      "    pl.figure(figsize=(8, 6))\n",
      "    pl.subplots_adjust(left=0.15, right=0.95, bottom=0.15, top=0.95)\n",
      "    pl.imshow(scores, interpolation='nearest', cmap=pl.cm.gist_heat)\n",
      "    pl.xlabel('$\\gamma$')\n",
      "    pl.ylabel('$C$')\n",
      "    pl.title('Heatmap for optimal: $C={}$ and $\\gamma={}$\\n'.format(grid.best_estimator_.C, grid.best_estimator_.gamma))\n",
      "    pl.colorbar()\n",
      "    pl.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
      "    pl.yticks(np.arange(len(C_range)), C_range)\n",
      "    pl.show()\n",
      "\n",
      "    print \"The best classifier for this run is: {}\".format(grid.best_estimator_)\n",
      "    return grid.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Determine Best Kernel Classifier\n",
      "\n",
      "Run through the different kernels for an SVC to determine the best. Possible kernels are:\n",
      "    \n",
      "* linear\n",
      "* poly \n",
      "* rbf\n",
      "* sigmoid\n",
      "* precomputed\n",
      "    \n",
      "We perform an exhaustive grid search across"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Get the optimal SVC\n",
      "\n",
      "Here we define the parameters for each of the kernel types and then run them through an exhaustive grid search. This is similar to the examples above where we ran through a list of $C$ and $\\gamma$ values but here the `GridSearchCV` takes care of all the details. This gets us our optimal classifier based on this exhaustive search."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup our ranges\n",
      "C_range = 10. ** np.arange(-4, 4)\n",
      "gamma_range = 10. ** np.arange(-4, 4)\n",
      "deg_range = np.arange(1, 5)\n",
      "coef_range = 10. ** np.arange(-4, 4)\n",
      "\n",
      "param_grids = [\n",
      "    { 'C': C_range, 'kernel': ['linear'] },\n",
      "#    { 'C': C_range, 'gamma': gamma_range, 'deg': deg_range, 'coef0': coef_range, 'kernel': ['poly'] },\n",
      "#    { 'C': C_range, 'gamma': gamma_range, 'kernel': ['rbf'] },\n",
      "#    { 'C': C_range, 'gamma': gamma_range, 'coef0': coef_range, 'kernel': ['sigmoid'] },\n",
      "]\n",
      "\n",
      "best_svcs = dict()\n",
      "\n",
      "# Get the best clf for each kernel type\n",
      "for param_grid in param_grids:\n",
      "    clf = get_optimal_svc(param_grid=param_grid)\n",
      "        \n",
      "    best_svcs[param_grid['kernel']] = { 'clf': clf }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Classify\n",
      "\n",
      "We have obtained our best classifiers above, so we classify the data with each. We will then compare how these classifiers worked with a ROC."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(clf=None, save_output=False):\n",
      "\n",
      "    # Use the best estimator from above and fit with our trained data and classes\n",
      "    clf.fit (labeled_svm, classes)\n",
      "\n",
      "    # Attempt a prediction\n",
      "    predicted_classes = clf.predict(unlabeled_svm)\n",
      "\n",
      "    if save_output:\n",
      "        # Creating an array to hold our new predicted values\n",
      "        out = np.zeros ((len(pred_class), unlabeled_svm.shape[1] + 1))\n",
      "        out[:,:unlabeled_svm.shape[1]] = unlabeled_svm[:][:]\n",
      "        out[:, -1] = predicted_classes [:]\n",
      "\n",
      "        # Save our new classifications out to a file\n",
      "        np.savetxt(\"classifications_predicted.csv\", out, delimiter=\",\")\n",
      "    \n",
      "    return predicted_classes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_roc(predicted_classes=None):\n",
      "    # Get our mask array of true values\n",
      "    predicted_correct_mask = np.array(unlabeled_classes == predicted_classes)\n",
      "\n",
      "    # Get the scores from the classifier\n",
      "    y_score = clf.decision_function(unlabeled_svm)\n",
      "\n",
      "    # Perform the ROC to get false-positives and true-positives\n",
      "    fpr, tpr, thresh =  metrics.roc_curve(predicted_correct_mask,y_score[:,0])\n",
      "    roc_auc = metrics.auc(fpr,tpr)\n",
      "    \n",
      "    return fpr, tpr, roc_auc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loop over each SVC type, classify, then compute the ROC\n",
      "for kernel in best_svcs:\n",
      "    clf = best_svcs[kernel]\n",
      "    \n",
      "    # Do the actual classifying\n",
      "    pred_class = classify(clf)\n",
      "    \n",
      "    # Compute the ROC\n",
      "    fpr, tpr, auc = compute_roc(pred_class)\n",
      "    \n",
      "    best_svcs[kernel]['fpr'] = fpr\n",
      "    best_svcs[kernel]['tpr'] = tpr\n",
      "    best_svcs[kernel]['auc'] = auc    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### ROC Curve\n",
      "Here we determine which classifier was the \"best\" via an ROC Curve"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the ROC for each kernel type\n",
      "pl.clf()\n",
      "fig = pl.figure()\n",
      "ax = fig.add_subplot(1,1,1)\n",
      "\n",
      "# Add a line for each kernel\n",
      "for kernel in best_svcs:\n",
      "    fpr = best_svcs[kernel]['fpr']\n",
      "    tpr = best_svcs[kernel]['tpr']\n",
      "    auc = best_svcs[kernel]['auc']\n",
      "    \n",
      "    ax.plot (fpr, tpr, label='{} (area = {0.2f})'.format(kernel, roc_auc)\n",
      "\n",
      "plt.legend(loc=4)\n",
      "plt.xlabel('False Positive Rate')\n",
      "plt.ylabel('True Positive Rate')\n",
      "plt.title('Receiver Operating Characteristic Curve')\n",
      "pl.show ()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Show some stats on the estimator with the best fit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diff_index = np.where(pred_class != unlabeled_classes)\n",
      "\n",
      "length_of_diff = len(diff_index[0])\n",
      "length_of_unlabeled = len(unlabeled_classes)\n",
      "\n",
      "diff_percentage = 1 - (float(length_of_diff)/float(length_of_unlabeled))\n",
      "\n",
      "print \"{:.2%}\".format(diff_percentage)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}